








<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-W87V548H1S"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

        gtag('config', 'G-W87V548H1S');
    </script>


    <!-- Google Adsense Tag -->
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9758578068902620" crossorigin="anonymous"></script>

    <meta charset="UTF-8">
    <title>ok97465 | Numba를 이용한 Cuda 프로그램 예제 </title>
    <meta name="description" content="Python에서 Numba를 이용한 Cuda 프로그래밍 예제를 소개한다.">
    <meta name="theme-color" content="#222222"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Numba를 이용한 Cuda 프로그램 예제">
    <meta property="og:description" content="Python에서 Numba를 이용한 Cuda 프로그래밍 예제를 소개한다.">
    <meta property="og:image" content="http://ok97465.github.io/ok_64x64.png">
    <meta property="og:url" content="http://ok97465.github.io">
    <script src="/assets/js/jquery.min.js"></script>
    <script src="/assets/js/popper.min.js"></script>
    <script src="/assets/js/bootstrap4.min.js"></script>
    <script src="/assets/js/header.js"></script>
    <script src="/assets/css/svg-with-js/js/fontawesome-all.js"></script>
    <link href="/assets/css/bootstrap4.min.css" rel="stylesheet">
    <link href="/assets/css/theme.css" rel="stylesheet">
    <link href="/assets/css/syntax.css" rel="stylesheet">
 </head>

<body>


<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-125307858-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>





<script type="text/javascript">
    WebFontConfig = {
        google: {
            families: ['Ubuntu::latin']
        }
    };
    (function () {
        var wf = document.createElement('script');
        wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
            '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
        wf.type = 'text/javascript';
        wf.async = 'true';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(wf, s);
    })();
</script>

<nav class="navbar navbar-expand-lg fixed-top navbar-dark">
    <div class="container">
        <a class="navbar-brand" href="/">ok97465</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse"
                data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup"
                aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
            <div class="navbar-nav">
                <a class="nav-item nav-link" href="/">/home</a>
                <a class="nav-item nav-link" href="/archive.html">/archive</a>
                <a class="nav-item nav-link" href="/tags.html">/tags</a>
                <a class="nav-item nav-link" href="/notes.html">/notes</a>
                <a class="nav-item nav-link" href="/about.html">/about</a>
            </div>
        </div>
    </div>
</nav>

    <div class="wrapper">
      <div class="content">
        <div class="container container-center">
          <div class="row">
            <div class="col-md-8 offset-md-1">
              <div class="article">
                <div class="card">
                  <h1><a href="/2019/07/190707_Numba_Cuda">Numba를 이용한 Cuda 프로그램 예제</a></h1>
                  <div class="post-meta">
                    <div class="post-time">
                      <i class="fa fa-calendar-alt"></i>
                      <time>07 Jul 2019</time>
                    </div>
                    <ul>
                      
                        <li><a href="/tag/python">python</a></li>
                      
                        <li><a href="/tag/cuda">cuda</a></li>
                      
                    </ul>
                  </div>
                  <div class="post-content">
                    
                      <ul class="toc">
  <li><a href="#numba를-이용한-cuda-프로그램">Numba를 이용한 Cuda 프로그램</a>
    <ul>
      <li><a href="#1-cuda-정리1">1. Cuda 정리[1]</a>
        <ul>
          <li><a href="#11-memory-hierarchy">1.1. Memory hierarchy</a></li>
          <li><a href="#12-host---device">1.2. Host &lt;-&gt; Device</a></li>
          <li><a href="#13-pinned-memory">1.3. Pinned Memory</a></li>
          <li><a href="#14-zero-copy-memory">1.4. Zero-Copy Memory</a></li>
        </ul>
      </li>
      <li><a href="#2-코드">2. 코드</a>
        <ul>
          <li><a href="#21-시간-측정-함수">2.1. 시간 측정 함수</a></li>
          <li><a href="#22-device-info-2">2.2. Device Info [2]</a></li>
          <li><a href="#23-sum">2.3. Sum</a>
            <ul>
              <li><a href="#231-sum-kernel-code-using-reduce-of-numba">2.3.1. Sum Kernel Code using reduce of numba</a></li>
              <li><a href="#232-sum-host-code-using-reduce-of-numba">2.3.2. Sum Host Code using reduce of numba</a></li>
              <li><a href="#233-sum-code-using-reduction">2.3.3. Sum Code using reduction</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#3-참고자료">3. 참고자료</a></li>
    </ul>
  </li>
</ul>
                    
                    <h1 id="numba를-이용한-cuda-프로그램">Numba를 이용한 Cuda 프로그램</h1>

<h2 id="1-cuda-정리1">1. Cuda 정리[1]</h2>

<p>이 절에서는 간단한 Cuda 프로그래밍 내용을 정리한다.</p>

<p><br /></p>

<h3 id="11-memory-hierarchy">1.1. Memory hierarchy</h3>

<figure>
    <img src="/assets/images/Cuda/memory_hierarchy.png" alt="Memory hierarchy" width="640" />
    <figcaption class="figure-caption">Memory hierarchy</figcaption>
</figure>

<ul>
  <li>Registers
    <ul>
      <li>Thread 간에도 공유되지 않는다.</li>
      <li>함수에서 선언한 변수를 저장한다.</li>
      <li>함수에서 선언한 변수의 크기가 Thread의 Register개수를 넘을 경우 Local 메모리에 위치한다.</li>
    </ul>
  </li>
  <li>Shared memory
    <ul>
      <li>__shared__를 이용하여 선언한다.</li>
      <li>Local, Global memory보다 지연시간이 적다.</li>
      <li>Block내의 Thread간 공유된다.</li>
      <li>__syncThreads()으로 Shared memory의 Coherent를 유지한다.</li>
      <li>L1 cache와 하드웨어를 공유하고 할당량은 cudaFuncSetCache함수를 이용하여 설정가능한다.</li>
    </ul>
  </li>
  <li>Local memory
    <ul>
      <li>Registers의 공간 부족으로 Cache에 위치한 변수</li>
    </ul>
  </li>
  <li>Constant memory
    <ul>
      <li>__constant__를 이용하여 선언한다.</li>
      <li>Global scope 선언되어야 한다.</li>
      <li>Kernel 실행 전 cudaMemcpyToSymbo로 값을 설정하여야한다.</li>
    </ul>
  </li>
  <li>Texture memory
    <ul>
      <li>Read Only</li>
      <li>2D Spatial Locality에 최적회 되어 있다.</li>
    </ul>
  </li>
  <li>Global memory
    <ul>
      <li>__device__식별자를 이용하여 선언하거나 Host에서 cudaMalloc을 이용한다.</li>
    </ul>
  </li>
</ul>

<hr />

<ul>
  <li>CUDA Variable and Type Qualifier</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Qualifier</th>
      <th style="text-align: center">Variable Name</th>
      <th style="text-align: center">Memory</th>
      <th style="text-align: center">Scope</th>
      <th style="text-align: center">Lifespan</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: center">float var</td>
      <td style="text-align: center">Register</td>
      <td style="text-align: center">Thread</td>
      <td style="text-align: center">Thread</td>
    </tr>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: center">float var[100]</td>
      <td style="text-align: center">Local</td>
      <td style="text-align: center">Thread</td>
      <td style="text-align: center">Thread</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>shared</strong></td>
      <td style="text-align: center">float var</td>
      <td style="text-align: center">Shared</td>
      <td style="text-align: center">Block</td>
      <td style="text-align: center">Block</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>device</strong></td>
      <td style="text-align: center">float var</td>
      <td style="text-align: center">Global</td>
      <td style="text-align: center">Global</td>
      <td style="text-align: center">Application</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>constant</strong></td>
      <td style="text-align: center">float var</td>
      <td style="text-align: center">Constant</td>
      <td style="text-align: center">Global</td>
      <td style="text-align: center">Application</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Salient Features of Device Memory</li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Memory</th>
      <th style="text-align: center">On/Off Chip</th>
      <th style="text-align: center">Cached</th>
      <th style="text-align: center">Access</th>
      <th style="text-align: center">Scope</th>
      <th style="text-align: center">Lifetime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Register</td>
      <td style="text-align: center">On</td>
      <td style="text-align: center">n/a</td>
      <td style="text-align: center">R/W</td>
      <td style="text-align: center">1 thread</td>
      <td style="text-align: center">Thread</td>
    </tr>
    <tr>
      <td style="text-align: center">Local</td>
      <td style="text-align: center">Off</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">R/W</td>
      <td style="text-align: center">1 thread</td>
      <td style="text-align: center">Thread</td>
    </tr>
    <tr>
      <td style="text-align: center">Shared</td>
      <td style="text-align: center">On</td>
      <td style="text-align: center">n/a</td>
      <td style="text-align: center">R/W</td>
      <td style="text-align: center">All threads in block</td>
      <td style="text-align: center">Block</td>
    </tr>
    <tr>
      <td style="text-align: center">Global</td>
      <td style="text-align: center">Off</td>
      <td style="text-align: center">†</td>
      <td style="text-align: center">R/W</td>
      <td style="text-align: center">All threads + host</td>
      <td style="text-align: center">Host allocation</td>
    </tr>
    <tr>
      <td style="text-align: center">Constant</td>
      <td style="text-align: center">Off</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">R</td>
      <td style="text-align: center">All threads + host</td>
      <td style="text-align: center">Host allocation</td>
    </tr>
    <tr>
      <td style="text-align: center">Texture</td>
      <td style="text-align: center">Off</td>
      <td style="text-align: center">Yes</td>
      <td style="text-align: center">R</td>
      <td style="text-align: center">All threads + host</td>
      <td style="text-align: center">Host allocation</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="12-host---device">1.2. Host &lt;-&gt; Device</h3>
<p>Host와 device의 변수가 동일한 파일에 선언되어 있어도 직접적인 참조는 불가능한다. 배열의 경우 cudaAlloc으로 할당된 포인터와 cudaMemcpy를 이용하여 Host와 device간 데이터를 교환할 수 있다. 하지만 Global scope에 __device__로 선언된 변수의 경우는 변수의 주소값과 cudaMemcpy를 이용하여 값을 전송 할 수 없다. 이런 경우에는 cudaGetSymboAddress를 이용하여 포인터를 얻어와서 cudaMemcpy를 이용하거나 cudaMemcpyToSymbol 명령을 이용하여 한다.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#include</span> <span class="cpf">&lt;cuda_runtime.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp">
</span><span class="n">__device__</span> <span class="kt">float</span> <span class="n">devData</span><span class="p">;</span>
<span class="n">__global__</span> <span class="kt">void</span> <span class="nf">checkGlobalVariable</span><span class="p">()</span> <span class="p">{</span>
   <span class="n">devData</span> <span class="o">+=</span><span class="mi">2</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span> <span class="p">{</span>
   <span class="kt">float</span> <span class="n">value</span> <span class="o">=</span> <span class="mi">3</span><span class="p">.</span><span class="mi">14</span><span class="n">f</span><span class="p">;</span>
   <span class="n">cudaMemcpyToSymbol</span><span class="p">(</span><span class="n">devData</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">value</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
   <span class="c1">// 위의 처럼 devData에 저장하거나</span>
   <span class="kt">float</span> <span class="o">*</span><span class="n">dptr</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
   <span class="n">cudaGetSymbolAddress</span><span class="p">((</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dptr</span><span class="p">,</span> <span class="n">devData</span><span class="p">);</span>
   <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dptr</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">value</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
   <span class="c1">//</span>
   <span class="n">checkGlobalVariable</span> <span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">();</span>
   <span class="n">cudaMemcpyFromSymbol</span><span class="p">(</span><span class="o">&amp;</span><span class="n">value</span><span class="p">,</span> <span class="n">devData</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
   <span class="n">cudaDeviceReset</span><span class="p">();</span>
   <span class="k">return</span> <span class="n">EXIT_SUCCESS</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="13-pinned-memory">1.3. Pinned Memory</h3>

<p>CPU에서 선언한 배열의 경우 Pageable 메모리 형태로 생성되므로 Device로 데이터를 전송 할때 Overhead가 생성된다. Host에서 사용할 메모리를 cudaMallocHost/cudaFreeHost를 이용하면 Pinned Memory(Non-pageable)로 선언되므로 Overhead를 줄일 수 있다. Pinned Memory가 많을 수록 host의 전체 성능이 저하 될 수 있으므로 시스템과 프로그램 상황에 맞게 조절되어야 한다.</p>

<figure>
    <img src="/assets/images/Cuda/pinned_memroy.png" alt="Pinned Memory" width="641" />
    <figcaption class="figure-caption">Pinned Memory</figcaption>
</figure>

<p><br /></p>

<h3 id="14-zero-copy-memory">1.4. Zero-Copy Memory</h3>

<p>일반적으로는 Host&lt;-&gt;Device간의 데이터를 직접 주고 받을 수 없지만 Zero-Copy memory로 선언된 영역에 대해서는 Host와 device가 접근할 수 있다. cudaHostAlloc/cudaFreeHost를 이용하여 선언/해제 할 수 있다.</p>

<p>Zero-Copy Memory의 장점은 다음과 같다.</p>

<ul>
  <li>Leveraging host memory when there is insufficient device memory</li>
  <li>Avoiding explicit data transfer between the host and device</li>
  <li>Improving PCIe transfer rates</li>
</ul>

<p>큰 배열의 경우 cudaMalloc을 사용하는 것이 훨씬 효율적이다.</p>

<p><br /></p>

<h2 id="2-코드">2. 코드</h2>

<h3 id="21-시간-측정-함수">2.1. 시간 측정 함수</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Timer</span><span class="p">:</span>
    <span class="s">"""클래스 생성 시점부터 소멸 시점까지의 시간을 출력한다."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">func_name</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s">'this func'</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">func_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">func_name</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_start</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">def</span> <span class="nf">__enter__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="kn">import</span> <span class="n">sys</span>
        <span class="kn">import</span> <span class="n">time</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'{} ==&gt;'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">func_name</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s">' '</span><span class="p">)</span>
        <span class="n">sys</span><span class="p">.</span><span class="n">stdout</span><span class="p">.</span><span class="nf">flush</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">process_time</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">self</span>

    <span class="k">def</span> <span class="nf">__exit__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="kn">import</span> <span class="n">time</span>
        <span class="n">time_end</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">process_time</span><span class="p">()</span>
        <span class="n">interval</span> <span class="o">=</span> <span class="n">time_end</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">time_start</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'Elapsed time: {:.8f} sec'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">interval</span><span class="p">))</span>
</code></pre></div></div>

<p><br /></p>

<h3 id="22-device-info-2">2.2. Device Info [2]</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pycuda.driver</span> <span class="k">as</span> <span class="n">drv</span>
<span class="n">drv</span><span class="p">.</span><span class="nf">init</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="s">'Detected {} CUDA Capable device(s) </span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">drv</span><span class="p">.</span><span class="n">Device</span><span class="p">.</span><span class="nf">count</span><span class="p">()))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">drv</span><span class="p">.</span><span class="n">Device</span><span class="p">.</span><span class="nf">count</span><span class="p">()):</span>

    <span class="n">dev</span> <span class="o">=</span> <span class="n">drv</span><span class="p">.</span><span class="nc">Device</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Device </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">dev</span><span class="p">.</span><span class="nf">name</span><span class="p">()</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">compute_capability</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="s">'%d.%d'</span> <span class="o">%</span> <span class="n">dev</span><span class="p">.</span><span class="nf">compute_capability</span><span class="p">())</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'  Compute Capability: </span><span class="si">{</span><span class="n">compute_capability</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'  Total Memory: </span><span class="si">{</span><span class="n">dev</span><span class="p">.</span><span class="nf">total_memory</span><span class="p">()</span> <span class="o">//</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s"> MB'</span><span class="p">)</span>

    <span class="n">dev_attr_tuples</span> <span class="o">=</span> <span class="n">dev</span><span class="p">.</span><span class="nf">get_attributes</span><span class="p">().</span><span class="nf">items</span><span class="p">()</span>
    <span class="n">dev_attributes</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dev_attr_tuples</span><span class="p">:</span>
        <span class="n">dev_attributes</span><span class="p">[</span><span class="nf">str</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="n">v</span>

    <span class="n">num_mp</span> <span class="o">=</span> <span class="n">dev_attributes</span><span class="p">[</span><span class="s">'MULTIPROCESSOR_COUNT'</span><span class="p">]</span>

    <span class="n">cuda_cores_per_mp</span> <span class="o">=</span> <span class="p">{</span><span class="mf">5.0</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
                         <span class="mf">5.1</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
                         <span class="mf">5.2</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
                         <span class="mf">6.0</span> <span class="p">:</span> <span class="mi">64</span><span class="p">,</span>
                         <span class="mf">6.1</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
                         <span class="mf">6.2</span> <span class="p">:</span> <span class="mi">128</span><span class="p">}[</span><span class="n">compute_capability</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'  Multiprocessors: </span><span class="si">{</span><span class="n">num_mp</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'  CUDA Cores Per Multiprocessor: </span><span class="si">{</span><span class="n">cuda_cores_per_mp</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'  Total CUDA Cores: </span><span class="si">{</span><span class="n">num_mp</span> <span class="o">*</span> <span class="n">cuda_cores_per_mp</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

    <span class="n">dev_attributes</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="s">'MULTIPROCESSOR_COUNT'</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">dev_attributes</span><span class="p">.</span><span class="nf">keys</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'  </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">dev_attributes</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Detected 1 CUDA Capable device(s) 

Device 0: GeForce GTX 850M
  Compute Capability: 5.0
  Total Memory: 2004 MB
  Multiprocessors: 5
  CUDA Cores Per Multiprocessor: 128
  Total CUDA Cores: 640
  ASYNC_ENGINE_COUNT: 1
  CAN_MAP_HOST_MEMORY: 1
  CLOCK_RATE: 901500
  COMPUTE_CAPABILITY_MAJOR: 5
  COMPUTE_CAPABILITY_MINOR: 0
  COMPUTE_MODE: DEFAULT
  CONCURRENT_KERNELS: 1
  ECC_ENABLED: 0
  GLOBAL_L1_CACHE_SUPPORTED: 0
  GLOBAL_MEMORY_BUS_WIDTH: 128
  GPU_OVERLAP: 1
  INTEGRATED: 0
  KERNEL_EXEC_TIMEOUT: 1
  L2_CACHE_SIZE: 2097152
  LOCAL_L1_CACHE_SUPPORTED: 1
  MANAGED_MEMORY: 1
  MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048
  MAXIMUM_SURFACE1D_LAYERED_WIDTH: 16384
  MAXIMUM_SURFACE1D_WIDTH: 16384
  MAXIMUM_SURFACE2D_HEIGHT: 65536
  MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 16384
  MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048
  MAXIMUM_SURFACE2D_LAYERED_WIDTH: 16384
  MAXIMUM_SURFACE2D_WIDTH: 65536
  MAXIMUM_SURFACE3D_DEPTH: 4096
  MAXIMUM_SURFACE3D_HEIGHT: 4096
  MAXIMUM_SURFACE3D_WIDTH: 4096
  MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046
  MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 16384
  MAXIMUM_SURFACECUBEMAP_WIDTH: 16384
  MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048
  MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 16384
  MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 134217728
  MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 16384
  MAXIMUM_TEXTURE1D_WIDTH: 65536
  MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 16384
  MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048
  MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 16384
  MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 16384
  MAXIMUM_TEXTURE2D_GATHER_WIDTH: 16384
  MAXIMUM_TEXTURE2D_HEIGHT: 65536
  MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65536
  MAXIMUM_TEXTURE2D_LINEAR_PITCH: 1048544
  MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 65536
  MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 16384
  MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 16384
  MAXIMUM_TEXTURE2D_WIDTH: 65536
  MAXIMUM_TEXTURE3D_DEPTH: 4096
  MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 16384
  MAXIMUM_TEXTURE3D_HEIGHT: 4096
  MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 2048
  MAXIMUM_TEXTURE3D_WIDTH: 4096
  MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 2048
  MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046
  MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 16384
  MAXIMUM_TEXTURECUBEMAP_WIDTH: 16384
  MAX_BLOCK_DIM_X: 1024
  MAX_BLOCK_DIM_Y: 1024
  MAX_BLOCK_DIM_Z: 64
  MAX_GRID_DIM_X: 2147483647
  MAX_GRID_DIM_Y: 65535
  MAX_GRID_DIM_Z: 65535
  MAX_PITCH: 2147483647
  MAX_REGISTERS_PER_BLOCK: 65536
  MAX_REGISTERS_PER_MULTIPROCESSOR: 65536
  MAX_SHARED_MEMORY_PER_BLOCK: 49152
  MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536
  MAX_THREADS_PER_BLOCK: 1024
  MAX_THREADS_PER_MULTIPROCESSOR: 2048
  MEMORY_CLOCK_RATE: 1001000
  MULTI_GPU_BOARD: 0
  MULTI_GPU_BOARD_GROUP_ID: 0
  PCI_BUS_ID: 1
  PCI_DEVICE_ID: 0
  PCI_DOMAIN_ID: 0
  STREAM_PRIORITIES_SUPPORTED: 1
  SURFACE_ALIGNMENT: 512
  TCC_DRIVER: 0
  TEXTURE_ALIGNMENT: 512
  TEXTURE_PITCH_ALIGNMENT: 32
  TOTAL_CONSTANT_MEMORY: 65536
  UNIFIED_ADDRESSING: 1
  WARP_SIZE: 32
</code></pre></div></div>

<p><br /></p>

<h3 id="23-sum">2.3. Sum</h3>

<p>GPU는 Code 최적화에 따라서 연산 시간에 많은 차이를 보인다. 간단한 SUM Code 조차도 CUDA에서 최적화 하는 것이 쉽지 않은 일이다(<a href="/assets/data/Numba_Cuda/gpu_sum_reduction.pdf">GPU_SUM.pdf</a>). 하지만 Numba의 reduce를 이용하면 SUM을 CUDA로 쉽게 Coding 할 수 있다.</p>

<p><br /></p>

<h4 id="231-sum-kernel-code-using-reduce-of-numba">2.3.1. Sum Kernel Code using reduce of numba</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">numba</span> <span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">from</span> <span class="n">numba.cuda</span> <span class="kn">import</span> <span class="n">to_device</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="nd">@cuda.reduce</span>
<span class="k">def</span> <span class="nf">sum_reduce</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</code></pre></div></div>

<p><br /></p>

<h4 id="232-sum-host-code-using-reduce-of-numba">2.3.2. Sum Host Code using reduce of numba</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">numpy</span> <span class="kn">import</span> <span class="n">float32</span>
<span class="kn">from</span> <span class="n">mkl_random</span> <span class="kn">import</span> <span class="n">standard_normal</span>

<span class="n">n_sample</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">26</span>

<span class="n">data</span> <span class="o">=</span> <span class="nf">float32</span><span class="p">(</span><span class="nf">standard_normal</span><span class="p">(</span><span class="n">n_sample</span><span class="p">))</span>
<span class="n">d_data</span> <span class="o">=</span> <span class="nf">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="nf">copy</span><span class="p">())</span>

<span class="k">with</span> <span class="nc">Timer</span><span class="p">(</span><span class="s">'Sum By CPU'</span><span class="p">):</span>
    <span class="n">sum_cpu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="nf">sum_reduce</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    
<span class="k">with</span> <span class="nc">Timer</span><span class="p">(</span><span class="s">'Sum By GPU'</span><span class="p">):</span>
    <span class="n">sum_gpu</span> <span class="o">=</span> <span class="nf">sum_reduce</span><span class="p">(</span><span class="n">d_data</span><span class="p">)</span>
    
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'CPU Result: </span><span class="si">{</span><span class="n">sum_cpu</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'GPU Result: </span><span class="si">{</span><span class="n">sum_gpu</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sum By CPU ==&gt; Elapsed time: 0.02662821 sec
Sum By GPU ==&gt; Elapsed time: 0.01087535 sec
CPU Result: 4220.08349609375
GPU Result: 4220.095703125
</code></pre></div></div>

<p><br /></p>

<h4 id="233-sum-code-using-reduction">2.3.3. Sum Code using reduction</h4>

<p><a href="/assets/data/Numba_Cuda/gpu_sum_reduction.pdf">GPU_SUM.pdf</a>의 예제를 Numba에서도 구현 해 볼 수 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Kernel Code
</span><span class="kn">from</span> <span class="n">numba</span> <span class="kn">import</span> <span class="n">cuda</span>
<span class="kn">from</span> <span class="n">numba.cuda</span> <span class="kn">import</span> <span class="n">to_device</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="nd">@cuda.jit</span><span class="p">(</span><span class="s">"void(float32[:], float32[:], int64)"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sum_kernel</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">idx_block_start</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">cuda</span><span class="p">.</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span>

    <span class="k">if</span> <span class="n">idx</span> <span class="o">&gt;=</span> <span class="n">n</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">stride</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="k">while</span> <span class="n">stride</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">stride</span> <span class="ow">and</span> <span class="p">(</span><span class="n">idx_block_start</span> <span class="o">+</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">stride</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">:</span>
            <span class="n">data</span><span class="p">[</span><span class="n">idx_block_start</span> <span class="o">+</span> <span class="n">tid</span><span class="p">]</span> <span class="o">+=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx_block_start</span> <span class="o">+</span> <span class="n">tid</span> <span class="o">+</span> <span class="n">stride</span><span class="p">]</span>
        <span class="n">cuda</span><span class="p">.</span><span class="nf">syncthreads</span><span class="p">()</span>
        <span class="n">stride</span> <span class="o">//=</span> <span class="mi">2</span>

    <span class="nf">if </span><span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">out</span><span class="p">[</span><span class="n">cuda</span><span class="p">.</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx_block_start</span><span class="p">]</span>

<span class="c1"># Host Code
</span><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">numpy</span> <span class="kn">import</span> <span class="n">float32</span>
<span class="kn">from</span> <span class="n">mkl_random</span> <span class="kn">import</span> <span class="n">standard_normal</span>

<span class="n">n_sample</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="mi">26</span>

<span class="n">data</span> <span class="o">=</span> <span class="nf">float32</span><span class="p">(</span><span class="nf">standard_normal</span><span class="p">(</span><span class="n">n_sample</span><span class="p">))</span>
<span class="n">d_data</span> <span class="o">=</span> <span class="nf">to_device</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="nf">copy</span><span class="p">())</span>

<span class="k">with</span> <span class="nc">Timer</span><span class="p">(</span><span class="s">'Sum By CPU'</span><span class="p">):</span>
    <span class="n">sum_cpu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">threadsperblock</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">blockspergrid</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">n_sample</span> <span class="o">/</span> <span class="n">threadsperblock</span><span class="p">)</span>
<span class="n">out_of_gpu</span> <span class="o">=</span> <span class="n">cuda</span><span class="p">.</span><span class="nf">device_array</span><span class="p">(</span><span class="n">blockspergrid</span><span class="p">,</span> <span class="n">float32</span><span class="p">)</span>

<span class="n">sum_kernel</span><span class="p">[</span><span class="n">blockspergrid</span><span class="p">,</span> <span class="n">threadsperblock</span><span class="p">](</span><span class="n">out_of_gpu</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">n_sample</span><span class="p">)</span> <span class="c1"># For Compile
</span>
<span class="k">with</span> <span class="nc">Timer</span><span class="p">(</span><span class="s">'Sum By GPU'</span><span class="p">):</span>
    <span class="n">sum_kernel</span><span class="p">[</span><span class="n">blockspergrid</span><span class="p">,</span> <span class="n">threadsperblock</span><span class="p">](</span><span class="n">out_of_gpu</span><span class="p">,</span> <span class="n">d_data</span><span class="p">,</span> <span class="n">n_sample</span><span class="p">)</span>
    <span class="n">out_from_gpu</span> <span class="o">=</span> <span class="n">out_of_gpu</span><span class="p">.</span><span class="nf">copy_to_host</span><span class="p">()</span>
    <span class="n">sum_gpu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">out_from_gpu</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'CPU Result: </span><span class="si">{</span><span class="n">sum_cpu</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'GPU Result: </span><span class="si">{</span><span class="n">sum_gpu</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Sum By CPU ==&gt; Elapsed time: 0.02695911 sec
Sum By GPU ==&gt; Elapsed time: 0.05354665 sec
CPU Result: 458.8485107421875
GPU Result: 458.8516845703125
</code></pre></div></div>

<p><br /></p>

<h2 id="3-참고자료">3. 참고자료</h2>
<p>[1] Professional CUDA C Programming  <br />
[2] Dr. Brian Tuomanen. (2018). Chapter3, Hands-On GPU Programming with Python and CUDA (39).</p>

                    

<div class="share-bar">
  <ul class="share-buttons">
    
      <li class="share-facebook">
          <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2019/07/190707_Numba_Cuda"
             target="_blank" title="Share on Facebook">
            <span class="fa-layers fa-fw fa-2x">
                <i class="far fa-square"></i>
                <i class="fab fa-facebook-f" data-fa-transform="shrink-8"></i>
            </span>
          </a>
      </li>
    

    
    <li class="share-twitter">
      <a href="https://twitter.com/intent/tweet?url=http://localhost:4000/2019/07/190707_Numba_Cuda&text=Numba를 이용한 Cuda 프로그램 예제" target="_blank" title="Tweet">
        <span class="fa-layers fa-fw fa-2x">
            <i class="far fa-circle"></i>
            <i class="fab fa-twitter" data-fa-transform="shrink-8"></i>
        </span>
      </a>
    </li>
    

    
    <li class="share-google-plus">
      <a href="https://plus.google.com/share?url=http://localhost:4000/2019/07/190707_Numba_Cuda" target="_blank" title="Share on Google Plus">
        <span class="fa-layers fa-fw fa-2x">
            <i class="far fa-circle"></i>
            <i class="fab fa-google-plus-g" data-fa-transform="shrink-8"></i>
        </span>
      </a>
    </li>
    

    

    

    
    <li class="share-envelope">
      <a href="mailto:?&subject=Numba를 이용한 Cuda 프로그램 예제&body=Python에서 Numba를 이용한 Cuda 프로그래밍 예제를 소개한다. http://localhost:4000/2019/07/190707_Numba_Cuda" target="_blank" title="Email">
        <span class="fa-layers fa-fw fa-2x">
            <i class="far fa-circle"></i>
            <i class="far fa-envelope" data-fa-transform="shrink-8"></i>
        </span>
      </a>
    </li>
    

  </ul>
</div>


                  </div>
                  
                  <div id="disqus_thread">
                    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
                    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
                  </div>
                  
                </div>
				<script type="text/x-mathjax-config">
                  MathJax.Hub.Config({
                    tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    processEscapes: true
                    }
                  });
                </script>
				<script type="text/javascript" async
                  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
                </script>
              </div>
            </div>
            <div class="col-md-3 hidden-xs">
              <div class="sidebar ">
  <h5><br><br><br>Recent Posts</h5>
  <ul>
    
    <li><h6><a href="/2020/02/200215_BinaryFileIO">Python에서 이진파일 (Binary file) 입출력</a></h6></li>
    
    <li><h6><a href="/2019/12/191204_Equidistant_points_on_ellipse">타원을 등간격으로 N 등분하기</a></h6></li>
    
    <li><h6><a href="/2019/10/191019_PrincipleOfCT">Computed Tomography (CT) 촬영의 원리</a></h6></li>
    
    <li><h6><a href="/2019/07/190707_Numba_Cuda">Numba를 이용한 Cuda 프로그램 예제</a></h6></li>
    
  </ul>
</div>

<div class="sidebar">
  <h5>Tags</h5>
  <ul class="sideBarTags">
    
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/cuda" data-toggle="tooltip" data-placement="right" title="2">
        <span>cuda</span></a><span>&nbsp(2)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/github" data-toggle="tooltip" data-placement="right" title="1">
        <span>github</span></a><span>&nbsp(1)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/jupyter" data-toggle="tooltip" data-placement="right" title="1">
        <span>jupyter</span></a><span>&nbsp(1)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/math" data-toggle="tooltip" data-placement="right" title="8">
        <span>math</span></a><span>&nbsp(8)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/matlab" data-toggle="tooltip" data-placement="right" title="2">
        <span>matlab</span></a><span>&nbsp(2)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/physics" data-toggle="tooltip" data-placement="right" title="3">
        <span>physics</span></a><span>&nbsp(3)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/pycharm" data-toggle="tooltip" data-placement="right" title="1">
        <span>pycharm</span></a><span>&nbsp(1)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/python" data-toggle="tooltip" data-placement="right" title="14">
        <span>python</span></a><span>&nbsp(14)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/signal_system" data-toggle="tooltip" data-placement="right" title="1">
        <span>signal_system</span></a><span>&nbsp(1)</span></h6></li>
    
    <li><h6><i class="fas fa-tag"></i><span>&nbsp&nbsp</span>
      <a href="/tag/sympy" data-toggle="tooltip" data-placement="right" title="4">
        <span>sympy</span></a><span>&nbsp(4)</span></h6></li>
    
  </ul>
</div>

            </div>
          </div>
        </div>
        
<!-- Add Disqus comments. -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'ok97465s-blog'; // required: replace example with your forum shortname
  var disqus_identifier = "/2019/07/190707_Numba_Cuda";

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


      </div>
          <footer class="footer-distributed">
      <div class="container">
        <div class="footer">
          <p>ok97465 &copy; 2018-</p>
          <h6>Follow me</h6>

<ul class="social-media">

  
    <li>
      <a title="ok97465 on Github" href="https://github.com/ok97465" target="_blank"><i class="fab fa-github fa-2x"></i></a>
    </li>
  

  
    <li>
      <a title="9015220/lowqualitydelivery on StackOverflow" href="https://stackoverflow.com/users/9015220/lowqualitydelivery" target="_blank"><i class="fab fa-stack-overflow fa-2x"></i></a>
    </li>
  

  

  
    <li>
      <a title=" on Instagram" href="https://instagram.com/" target="_blank"><i class="fab fa-instagram fa-2x"></i></a>
    </li>
  

  
    <li>
      <a title=" on Last.fm" href="https://www.lastfm.com/user/" target="_blank"><i class="fab fa-lastfm fa-2x"></i></a>
    </li>
  

  
    <li>
      <a title="feed.xml RSS" href="/feed.xml" target="_blank"><i class="fas fa-rss fa-2x"></i></a>
    </li>
  

</ul>

        </div>
      </div>
    </footer>

    </div>
  </body>
</html>
